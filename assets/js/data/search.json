[ { "title": "Kugwidwa Ufiti in a Serverless Database", "url": "/posts/kugwira-mfiti-in-a-serverless-database/", "categories": "Databases, Data Engineering", "tags": "snowflake, databases, data engineering, data warehouses", "date": "2024-05-21 18:00:00 +0200", "snippet": "In late 2022, we started exploring Snowflake as an alternative to our existingdata warehouse solution at my usual place of work. Before then we were astrictly Microsoft SQL server house. Sequel Ser...", "content": "In late 2022, we started exploring Snowflake as an alternative to our existingdata warehouse solution at my usual place of work. Before then we were astrictly Microsoft SQL server house. Sequel Server was adopted in the firstplace because the organisation already had a lot of expertise around it.Adopting it as a data warehouse was rather natural. It did serve the organisationquite well for a number of years. Overtime the number of users of the warehousegrew. At the same time, we were also producing more data than before. Totalnumber of datasets was closing in to over a thousand and some of these datasetsgrew quite large, hitting hundreds of millions of rows.Some of the problems we experienced included tables locking up because ofdata pipelines trying to update the tables, whilst a number of users were tryingto access those same tables at the same time. Also the server would just crawlto a halt because of too much, that was too heavy, happening.We needed a change… From our exploration we ended up landing onSnowflake. Snowflake could quite easily address allthe problems that we had. We did eventually migrate to it and I can tell youthat performance hasn’t been a problem at all, again. Snowflake is blazinglyfast for analytics. We are able to do far more than we were able to do on SQLServer with very little effort. The closest we had to performance problems wasthis one time when we found ourselves executing a lot of queries (thousands) atthe same time, so Snowflake had to queue up some of these queries. The fix wasrather easy; with a click of a button we were able to scale up our warehouse tohandle the load.Snowflake did solve our performance problems but it did come with its owncruft. The costs of running Snowflake were rapidly rising by the day withmore and more adoption within the organisation. This increase in costs didnot necessarily come from pushing Snowflake to the max but rather inefficientuse of it. We basically just did a lift and shift operation of our workand how we worked on SQL Server to Snowflake. That ended up unnecessarilycosting us heavilly.Moving to a Serverless Database requires a Change of Behaviour “Modern solutions require modern problems” (Anonymous Dude on the Internet)Snowflake is a serverless data warehouse. There is no server for us tomanage in a sense. Snowflake just gives us a choice on how much computewe need. We don’t need to be experts on the internal workings of Snowflakelike we were required with SQL Server. We don’t need to tune servers toget the most out of them. I haven’t even thought about where to put anindex in over a year now. It’s brilliant, however there is a catch…Costs in serverless don’t work the same as they do in an environmentwhere we manage our own servers.When we pay for a server on Cloud Africa forexample, what we get is exclusive ownership of the server for the term ofthe contract we have gotten into. Serverless, on the other hand, doesn’twork quite like this. We pay for compute time, not a server. In otherwords, we kind of pay for how much we do. When we own a server, wecan do as we please, so long as the server can handle it. On serverless,we have to cough up more money to do as we please. And those costs dorise rapidly.On Snowflake, we buy credits which are time-bound. Credits can onlybe used within a given time period. Credits translate to compute hours.One credit is consumed for an hour of compute on the smallest warehousepossible (instead of servers you think of warehouses on Snowflake). Thebigger the warehouse, the more credits consumed per hour. Here is thecatch, one hour of compute time does not necessarily mean that you havedone an hour of work. That just means the warehouse was up/available foran hour. We can do more than an hour of work (multi-tasking) in that timeor less. We can end up paying for an hour of compute when we have actuallyperformed less than a minute of work and this was what was happening tous.Here is what happens… When you send a query to Snowflake, a warehousepowers up to handle your query. When done with the query, the warehousedoesn’t power down immediately. It waits for some time in casemore queries come in. We had this timeout set to 10 minutes initially.The warehouse would then most times wait for a number of minutes for aquery that may or may not come, and it when it does, the 10 minutes getsextended. In effect, we would have a warehouse up hours on end,doing nothing. We were paying money to literally do nothing!!! We dideventually come to reducing the timeout time to the minimum possible1 minute. This improved things for a while but time still did catch upwith us again.The root cause of the problem we had stemmed from the fact that we didnot change our behaviour moving from SQL server to Snowflake. When wewere working on SQL server, we would schedule our pipelines to runwhenever without thinking about it too much. For example, I would come inand configure my pipeline to run every hour starting from now (time is18:23 now). Somebody else comes in schedules their pipeline to run every30 minutes starting from 18:31 and so on. Scale this behaviour to dozensof individuals, it quickly becomes a problem (we have data engineers,analysts, and scientists all scheduling their ETL and analytical pipelines).Soon, we find ourselves in a situation where the warehouse doesn’t get topower down. On average, we find that each of these individuals is doing verylittle work. Maybe just a query that checks if there is new data. It takesless than a second to run but in effect costed us a minute of compute.To make matters worse we had people building APIs and using Snowflake astheir database. There are two problems with APIs: 1) their traffic is notpatterned (akin to the scheduling of pipelines we were doing) and 2)healthchecks (these can come in every second).Our problem was not serverless per se, our problem was that we operatedin a serverless environment as if we were in a self-managed environment.In a self-managed environment we can afford to be wasteful because wedon’t really need to think about costs relative to what we were doing.What we were doing only mattered when we tripped on each other. I amslowing down the database because of a heavy query that I am runningfor example.Measuring User/Role/Query Impact on a Serverless databaseWhen we figured out that our behaviour was the problem, the next stepwas to be able to detect when someone was behaving like a savage. For thiswe ended up using tools we already had in our utility belt but we endedup applying them differently. Again as the behaviour thing, we couldn’tsimply carry what we knew from SQL server over to this.If something was going wrong on SQL server, we would look for the slowestqueries or look for a query that was running too many times in a day.Looking at these two things independently worked on SQL Server but itturned out to be useless on Snowflake for the most part. A query thatruns continuosly for 3 hours in a day could end up costing us less thana query that runs for a total of 30 seconds in a day. A query that runsonce a day could cost us more than a query that runs 10,000 times ina day. The runtime and scheduling of a query both mattered. It was neveran either - or situation. Sometimes our biggest cost would come fromthe 3 hour query and sometimes it could be the query that runs 10,000times depending on how it was scheduled. A query could run 10,000 timeswithin the same minute. The cost of that is less than that of a querythat runs continuosly for 5 minutes. At the same time if the 10,000runs are spread out throughout a day, the cost would tilt the otherway round. We needed a measure that could capture both of thesesituations. The measure also had to be applicable to various elementslike roles, users, and queries.We ended up on a measure we called relative cost (for lack of a bettername - English is not our first language, Chidzungu chinabwera pa botiichi). It combines both the number of times a query is executed andhow long the query takes on each run. The scale of this measure is roughlybetween 0 and 1 (it can exceed 1 but we don’t care about that excess -it adds no more meaning). The closer to 0 the value is then the lower theimpact on the data warehouse and the closer to 1 it is, then the user, role,or query in question kept the warehouse up for about 100% of the time.The logic behind the measure goes something like this: Walk every minute of an hour For each minute check if there is a query started by a particularuser or role Sum up the each of these occurrences and total execution time Divide the total by 60 (60 minutes in an hour) NOTE: If a query executes in every minute for less than a secondthen after the summing up you end up with a value that’s about 1.Same as if you only have a single query that runs for the entirehour.The query we were running for this looks something like:WITH roles_relative_cost ( SELECT warehouse_name, role_name, user_name, TO_CHAR(start_time, 'YYYY-MM-DD HH:MI:00')::DATETIME AS minute, (DATEDIFF(MINUTES, MIN(start_time), MAX(end_time)) + 1) / 60 AS relative_cost, COUNT(query_hash) AS num_queries, SUM(execution_time) AS execution_time FROM snowflake.account_usage.query_history WHERE start_time BETWEEN DATEADD(DAY, -1, CURRENT_DATE) AND CURRENT_DATE GROUP BY ALL)SELECT TO_CHAR(minute, 'YYYY-MM-DD')::DATE AS date, COALESCE(warehouse, 'Unknown') AS warehouse_name, role_name, user_name, SUM(relative_cost) / 24 AS relative_cost, SUM(num_queries) AS num_queries, SUM(execution_time) / 60000 AS execution_time -- Express in minutesFROM roles_relative_costGROUP BY ALLORDER BY 5 DESC, 6 DESC, 4 DESC;The query above gives each role/user combo’s relative cost. Onse witha cost above 0.5 ndi athakati. By multiplying the cost to 24 we canget a rough estimate of how long they kept the warehouse on. We havethis query and a couple of variations running in n8nevery morning to produce a reports showing the most expensive queriesand other information.Jah man…" }, { "title": "My First AOC Experience [DRAFT]", "url": "/posts/my-first-aoc-experience/", "categories": "Programming", "tags": "programming, crystal", "date": "2024-01-09 12:00:00 +0200", "snippet": "Last year I decided to try out Advent of Code (AOC). I didn’t get to finish the wholething. From what I managed to do though, I had fun and I believe that I levelled up a bitin the language I used....", "content": "Last year I decided to try out Advent of Code (AOC). I didn’t get to finish the wholething. From what I managed to do though, I had fun and I believe that I levelled up a bitin the language I used.My initial goals for AOC 2023 were to build more familiarity with two languages:Crystal and Scala. Crystal because that is what I have decided to make as my language ofchoice for any personal experiments/work and Scala because … well I professionally workas a data engineer and Scala rules in data engineering to an extent. Both of theselanguages I had some experience with but not enough to claim fluency. I was somewhatskeptical that I would be able to meet this goal because of the thoughts I had about AOC.I was expecting the time constrained and basic kind of questions that are mostly givenin a lot of online coding tests. In those type of tests, a short amount of time to solvethe problem is given and in most cases the problems themselves do not require coming upwith an overly elaborate solution (say one that requires some fancy data structure).Under such constraints, I don’t think there is enough leeway for me to explore what I cando with a programming language. However, what I learnt from AOC is you can meet whatevergoals you have set out for yourself depending on how you approach it.In AOC, you aren’t time bound unless you go into it to compete. The problems can also becomplex enough to require some elaborate algorithms plus data structures. This gives a lotof freedom for experimentation in whatever language you choose to go with. Anyways, I amnot trying to sell AOC to anyone, I just want to talk about some of the things I run intothat I thought were cool and maybe worth sharing (to a Malawian audience???). If you intend to do the AOC 2023 and don’t want any spoilers, don’t proceed.Cycles … Cycles … Cycles …There were a couple of questions that involved some sort of cycle or loop.You could be traversing some paths trying to find the shortest path among them. However,some of them happen to be cycles. If you aren’t careful you could find your program runningforever because it is locked in a cycle. Or you could have something like given these loopingpaths of different lengths that have the same starting point, how many steps does it take forall of them to converge on that starting point again. I am paraphrasing here but I hope youget the point.I found the first kind of problem a bit straightforward to handle. I would build a directedgraph as I traverse the possible paths. Cycle detection was happening at the point ofadding a vertex to the graph. I would check if the vertex and its edge already exist inthe graph. You can see an example of this solutionhere.I was representing my graph as a map of vertices to edges.alias Point = Tuple(Int32, Int32)graph = {} of Point =&gt; Array(Point)The Point is a vertex and then I have it mapped to an array of other Points. Thoseother points are essentially edges. I am not maintaining any weights or any otherattributes for each edge hence why I just have the Point. Having the graph as a mapmakes looking up vertices very easy. There are other ways to represent graphs(e.g. tree-like structures) but if you are going to be randomly looking up verticesin the graph then a map should be one of the first base data structures to reach out for. Shout out toFloyd’s algorithm.I didn’t get to use it but I had it locked and loaded, ready for firing after I sawthat cycles and graphs were a thing. It can be used for cycle detection and it is moreefficient than what I was doing above.The other place where a cycle was involved was around something like finding theconvergence point of multiple looping paths that have the same starting point. The naiveway of solving this problem is to simultaneously traverse all paths until all pathsconverge at the starting point. For the input that was I provided the solution ended up being21,366,921,060,721 steps. There was no way I was brute-forcing my way to that answer. Trywriting a program that just counts from one to a trillion and see how long it takes tocomplete. If you are feeling lazy, let’s do some quick maths… Assume that it takes amillisecond to increment your counter by one. To get to a thousand, you need a second.Within a minute you would have done sixty thousand. To get to a billion you would need277 hours. To hit a trillion, well about 31 years if my math is correct.Of course, I didn’t know the scale of the problem I was facing. I didn’t know that answerwould be that high so I tried the naive way and I gave up after an hour orsomething with my computer running super hot that ndikanatha kumayiwotha ngati mbaulabwinobwino. I was totally lost on how I could solve this. I humbled myself and looked updiscussions on this problem onr/adventofcode.Turns out that some really smart people quickly figured out that the solution isLowest Common Multiple (LCM). Seems obvious I know, probably that’s because of how I havephrased the problem here but the actual AOC problem, wasn’t so clear. You had to put insome work to first figure out that there were cycles. The sizes of each of those cycles hadto be determined and then calculate the LCM of the cycles. Since I already knew that therewere cycles involved, I just wrote some basic code to find the cycle size. You can see howI implementedthat here.If you are confused as to why LCM is a solution to this problem, you just have to remindyourself of what an LCM is. An LCM is the lowest number that a bunch of other numberscan all divide without a remainder. The LCM is the inital point of convergence for allnumbers involved.Gat damn these floods!!!Say you have to implement something like selection of all point enclosed by a pathcreated by the path tool in Gimp, how would you go about it? In other words, givena closed path, you have to identify all the pixels that are enclosed by that path.This took me a while to solve and I was able to get through it only thanks to ChatGPTfor reminding me of a super simple solution to the problem.After overthinking the problem too much I opted to implement aflood fill. With flood fill you more or lesscolour the region contained within the closed path a different colour from the regionoutside. You start with a cell (pixel), give it a colour and then move onto the next oneand give it the same colour if it’s not on the path and so on… My idea was that I justcount the number of cells (pixels) with the inside colour when done doing the colouring.I run into some issues with this approach. First, it was near impossible for me to justlook at the input I was given and say this is the inside of the loop and that’s theoutside. Still, with this solution I should have still got the answer. I would have hadtwo values, one for the inside and another for the outside. But then there was a seconproblem, there were some hard edge cases that I failed to handle with my flood fill.Trying to add a fix for the edge cases proved to be too much of a challenge so I cavedand asked ChatGPT what algorithm it would use to determine whether a point is withina closed curve. ChatGPT was like “point-in-polygon.”My immediate reaction was like, “The f#$k???” I asked for more details and ChatGPTdescribed pretty much describedRay Casting.I can only blame years of CRUD for wiping this simple algorithm from my brain. Thisis something anyone with a basic experience with computer graphics should be familiarwith. Anyways, I went ahead and implemented the solution. Handled a couple of weirdedge cases and I was home and dry. With ray casting, what you are effectively doingis drawing a straight line through the image to a particular cell (pixel). Then youcount how many points on the line intersect the closed path. If that number is oddthen you have a point on the inside, otherwise it’s on the outside. You can referto the diagram below for a visual representation:........|....................|...............************.........*....|.....*.........*....$.....*.........*..........*.........************................................................From the image above, the point being checked is marked $. As you can see thereis a ray that’s been cast from the top of the image straight to it. It crosses thepath demarcated by *s at one point only. One is odd thus $ must be within theclosed path. My implementationheregoes through every point on the image and checks if a ray cast as above crossesthe path at an odd number of points. There was a hard edge case I had to handlefor points that lie below turns (corners). Consider the following:..............|....................|.........************.........*..........*.........*..........*.........*..........*.........************....................$...........................If you naively do a ray cast you might end up with a value of 5. The ray “crosses”the path at 5 points therefore you may wrongly come to the conclusion that the pointis within the path. I worked around this problem by keeping track of the contiguousparts of the path above the point and counting them. If a contiguous path forms aturn (ie. it turns back) then that counts as two, else as one...............|...... .............|.......................|...... ..........................************...... ...***********............*..........*...... ...*.........*............*..........*...... ...*.........*****........*..........*...... ...*.........$...*........************...... ...***************...................$...... ............................................ .......................Forms a turn == two Doesn't form a turn == oneDon’t force matters or matters will force youI was supposed to talk about brute force here koma ndatopa. Message to carry homehere is that brute force sometimes isn’t the way. It can cause you a whole lottapain. I have touched upon this to some extent under nkhani yama cycles. Mwina somedayndizakamba zambiri." }, { "title": "Don't Settle Too Early [DRAFT]", "url": "/posts/dont-settle-too-early/", "categories": "Programming", "tags": "programming", "date": "2023-11-08 20:00:00 +0200", "snippet": "A couple of days ago there was a debate on programming languages (technology in general).in one of our local developer groups. The initial proposition that was made was kind ofsomething like PHP is...", "content": "A couple of days ago there was a debate on programming languages (technology in general).in one of our local developer groups. The initial proposition that was made was kind ofsomething like PHP is a dead language. Everyone should switch to Python instead, well… because AI (I don’t agree with 99% of the arguments that were made in support of this,story for another day though…). The debate progressed into a different topic; somethingto do with specialisation. It was more around the idea of specialising in one or twoprogramming languages and more or less ignoring everything else. You don’t need to learnRust or Go or [insert some language/framework/tool here] because: There is no demand for those technologies in Malawi… The kinds of problems we have in Malawi don’t really need those technologies,you can easily solve our problems with PHP Your users don’t care about the technology used in developing your service People only learn/use these technologies to show off to other developersWhilst I do understand these arguments, I think this kind of reasoning could hamper aperson’s growth as a software developer especially if the person is just starting out.I also believe it’s not good for our developer industry in general. Before proceeding, Ineed to drop a disclaimer (people these days are easily offended - too many snowflakesthat take opposing views as a personal attack): I am not against specialisation… I am against the thinking that specialisationmeans ignoring everything not within the immediate scope of one’s area ofspecialisation (e.g. Python is more than enough, I don’t need to learn PHP…). Everything in this article are just my personal opinions. Most of these are comingfrom my experience and the arm-chair programming philosophy I do from time to time.A different language can make you a better developer at your language of choice “LISP is worth learning for a different reason — the profound enlightenment experienceyou will have when you finally get it. That experience will make you a betterprogrammer for the rest of your days, even if you never actually use LISPitself a lot.”Eric S. Raymond - How To Become A Hacker When I was starting my journey into programming, one of the first articles I went throughwas ESR’s Hacker How-To. One thing I learnt from it wasthat there are tools out there that I will learn for the sake of learning. I might neveruse the tools themselves in any real life projects but the lessons I learn from thosetools will help me be better at whatever tool I stick to. This idea is summed up in thequote above. You may agree with it or not but there is an element of truth in it.There are things out there whose sole purpose in our lives is to teach us how to makebetter use of the things we already have.Primarily, I am a Python developer. I started programming in this language in 2010. Overthe years there are certain concepts that I now use that only became clearer when I lookedat them from a different programming language. The easiest example I can give (and I usuallydo give) is on use of interfaces. For a time, interfaces were a concept that I had heard ofand could explain to an extent but I never really thought through as I was programming.Interfaces didn’t really factor into the decisions I made when structuring my programs.I then learnt a bit of Java in something like 2012 or earlier. I started learningJava mostly because I wanted to start contributing to Zangaphee Chimombo’sjBawo (I was passionate about board games, freesoftware, and programming - these were some of the best days of my life). Unfortunately,I never got to contribute to jBawo beyond bug submissions. I didn’t learn enough Java tobe writing it competently. Swing, AWT, and ma threads anandigudubuza sizibwana. I didlearn about interfaces though and I started to think about them a little more in thedesign of programs in Python.Here(and here too)are examples of how that usage looked like. You can also see it below: class BaoAgent(object): '''An interface used to communicate with a bao arbiter or player.''' def new_game(self, field, **kwargs): pass def message(self, message, from_=None): '''Send message to agent. from_ is the id of the original sender, if None then the message was sent by the arbiter ''' pass def move(self, m): '''Send move made to agent.''' pass # And then I would later come up with implementations like: class Player(BaoAgent): def new_game(self, **args, **kwargs): pass def message(self, *args, **kwargs): ... class Malume(BaoAgent): \"\"\"AI player\"\"\" ... class NetworkedPlayer(BaoAgent): ...Whether this was good practice is debatable … Python now hasABCs andprotocols that kind of provide native supportfor the same thing. Learning about how to properly use interfaces in Java gave mean extra tool that I used in designing my Python programs. So, was learning Java a wasteof time since I never got to use it much? I don’t think so… In my opinion, learningJava made me a better programmer overall.Later on I picked up Ruby, mainly out of peer pressure from madolo who were working atBHT back then. These people made sure we know how beautiful Ruby was. I dove into it tosee this beauty that they spoke of and I can report back that it truly is there.If you are just curious about programming languages, check it out sometime. One of thethings I learnt in Ruby was this concept called duck typing. Basically, duck typing canbe summarised by the phrase “if it quacks then it’s a duck.” Learning this made merealise that I don’t necessarily always need to be explicit about interfaces. I stillhave to think about them but in a dynamic language like Ruby, I don’t need to be superexplicit about them. I ended up going back to a style similar to what I was doing initiallyin Python but with a lot more thought put into the structure of my programs. As I have grown older – don’t know if it’s kukalamba chabe, I have come to preferstatically typed languages. Even though I still use Python, I make heavy use of Python’stype annotations. I am a bit more explicit with my interfaces usingprotocols. Protocols providestructural subtypingfor Python type annotations. This to an extent is duck typing with some extra steps.If you are familiar with Go’s approach to interfaces, that’s pretty much what protocolsare.If there is one thing you can take from this is that you should not keep yourself fromlearning more languages, frameworks, etc beyond what you primarily use. Some ideas areeasier to grok from a different perspective than the one your immediate tools give you.Exploring different tools can help you see the limitations of your current toolsMost tools have communities around them. These communities tend to advocate differentapproaches to problem solving. Take web programming for example: if I want to builda web application in languages like Go, Erlang, or Clojure, chances are that I willhave to make choices about things like: What server should I use? What of a router? To ORM or SQL builder or just raw SQL? What HTML templating engine should I use? How am I running this application in production?For I to be able to make competent decisions about any of these things, I need tohave an idea of what all those things are and why they exist. Having this knowledgeallows me to choose among various options and more importantly, I know what problemsthese things are helping me avoid. I believe that the knowledge required for me to beable to make such decisions makes me overall a better developer compared to howI would be if I was never pushed to learn these things at some point.Consider HTML templating engines, I know that most templating engines help me avoidXSS issues in some way. Almost all engines automatically escape HTML for me. To optout of this I need to be explicit about my intentions. The idea that XSS isa potential issue was only hammered down into my brain because of my exposureto templating engines.Of course, there are full blown frameworks in those languages that make these decisionsfor you. However, you will mostly find that developers prefer making these decisions forthemselves. The culture around these languages will you push into becoming a developerwho strives to have a deeper understanding of the various bolts and nuts that go intobuilding a web application. Contrast this with web development in something like Python.From my experience of web development in Python, the culture there tends more towardsbatteries included web frameworks like Django. If your first experience in web developmentis in Python, you will likely be driven towards Django (there are microframeworkslike Flask and FastAPI but I hardly see these recommended to beginner web developers).With Django, you will just get all the pieces that you need to build a web applicationwithout getting much context as to why those pieces are there. So, out of the gate,you will likely learn more about what goes into a web application if you picksomething like Go over Python. With Python you will likely be able to put togethersomething of significant complexity faster than you would in Go. On the flip side,by the time you get to build something with similar complexity in Go, you willhave a deeper understanding of web development in general. If you start withPython, learning web development in Go later might give you a fuller understandingof all the components that make up Django. If all you are is a Go web developer,web development in Python might teach you how to build web applications quickly.In sum, what I am trying to say here is that after you learn and getcomfortable with your raw dog PHP, look at Laravel. When you are donewith Laravel maybe look at web development in Javascript or some otherlanguage. You don’t have to become an expert in the other environments.Know enough to be dangerous as some say. The further you go, the deeperyour understanding of web development becomes. The more you know, theclearer the limitations of your current tools (and the ones you are exploring)become to you. This goes beyond web frameworks, things as simple as style areapproached differently in different communities. You can’t learn everything but then you just can’t settle on one thing andignore everything else. Try scouring places like dev.toor diff.blog to learn about what people are doing out there.If you see something interesting, check it out sometime. Quickly browsing a toolsdocumentation is more than enough in my opinion. You can deep dive if you seesomething really interesting.Demand may simply reflect the lack of information not people’s needs“You only really need to know PHP (and HTML… and Javascript… and CSS) to getdeveloper gigs in Malawi because that’s what the market demands.” That’sa sentiment you will hear quite often among the software development elders in Malawi.That’s a fact and I can’t argue against it. However, I don’t believe this shouldbe the primary reason that entices you to specialise in PHP or whatever is in demand.Consumers (ie. people looking for software development services) will only demand forwhat they know, not necessarily what they need. We as the supply side of web developmentservices in Malawi have convinced the demand side that PHP is the (only?) way to build webapplications. We have more information than the demand side has. If the demand sidehad perfect information about what options are available, would it still alwayschoose PHP? I personally don’t have a definite answer but looking at the top developeremployers in Malawi whose demand for developers is largely driven by other developers,I am inclined to say no!!!We almost all tend to argue that we are developing applications for a resourceconstrained country. If that’s truly the case then what the Malawi market needsare languages (tech) that minimise operating/running costs and not necessarilyjust development costs. The way I see the market in Malawi is that the servicesoffered are optimised for developer productivity (PHP ranks very high when itcomes to shipping applications quickly). I am certain that for any substantiallycomplex application in PHP or Ruby, we can build an equivalent in something likeRust or Go. The Rust/Go application’s operating costs will likely be a fraction ofthe PHP/Ruby application. If the customer who we are building the application forhad access to this information, would they still be okay with PHP? Say you pose toyour customer with a question like “With some increase in the development cost,I could cut down on how much you will have to pay monthly in keeping this applicationrunning for the rest of its life?” You would probably have to provide detailedinformation on how much the development cost increases and how much the operatingcosts decrease.To some extent, I believe the high demand for PHP on our developer marketis a reflection of lack of information on the demand side.To be continued…" }, { "title": "A Case for Paper Based Workflows", "url": "/posts/A-Case-For-File-Based-Workflows/", "categories": "Programming", "tags": "programming, mpanje", "date": "2023-10-19 20:00:00 +0200", "snippet": "A common thread I find in most software developers’ product pitches in Malawi (Africaas a whole too to some extent) revolves around replacing paper based workflows withalmost one hundred percent di...", "content": "A common thread I find in most software developers’ product pitches in Malawi (Africaas a whole too to some extent) revolves around replacing paper based workflows withalmost one hundred percent digital workflows. Complete elimination of paper in anyworkflow is seen as a big plus. Whilst I do understand where this line of thinking iscoming from, I find that more often than not these products don’t fit the Malawianenvironment quite right. Almost every single Malawian has ever found himself/herselfunable to access some public service because there is “no network” or “no electricity.”If you go back to a time when these public services were run on fully manual paper basedworkflows, you will hardly find cases where this sort of thing happened. To some extentI think it’s arguable that these public services have overall regressed despite theefficiency gains from the digitization.Back in 2018, I joined Baobab Health Trust. I was placed on the team that was responsiblefor an application called ART. ART was/is an application that is used in a number (200+)of clinics in Malawi to deliver HIV/AIDS Antiretroviral treatment. The first iteration ofART was developed cha m’ma thu sauzandemu. There are many lessons I learnt when I wasworking on ART. Of all the lessons, one particularly stands out for me. I was made toalways think of situations when ART fails. Should clinics come to a complete hault becauseof a bug in the application? Or perhaps there is no electricity and the servers are off,should the clinic stop running? The obvious answer here is no; the clinic must keeprunning one way or the other. ART was built from the ground up to work alongside a paperbased workflow. ART enhances the paper based workflow instead of eliminating it.In situations where ART fails, clinics are still able to operate (albeitnot efficiently) following the paper based workflow.How exactly does ART enhance a paper based workflow?When using ART, clinics are required to maintain a physical file for every patient.The physical file contains a good amount of the patient’s medical history that’s enoughfor a medical practioner to use in treating the patient. So, when a patient comes to aclinic and is searched for in ART, the application provides the file number of thepatient’s physical file. It also provides an additional optional feature that helps inlocating where exactly that file is located in the shelves where files are stored.An ART workflow is broken down into multiple stages. At each stage ART prints out somesmall labels providing a summary of all the information collected and generated at thatstage. The labels are then pasted on a page in the physical file. Essentially, a copy ofpart of the data in ART is made available in physical form. The data in the physical fileis enough for a clinician to use in attending to a patient during ART down times.For example, when a patient is prescribed some medication, a label having the prescriptionis printed out. The prescription label is then pasted in the patient’s file. The next timethe patient comes and ART is not available, information about the patient’s lastprescription is available to the clinician to use if needed. As you can see, ART has noteliminated something the clinician already does but has rather enhanced it. In the absenceof ART, the clinician still has to write that prescription down in the file. ART justmakes that process easier. ART is simply making the clinician more efficient at his/herwork, not completely eliminating it.To keep things in sync with the physical files after an ART outage, ART provides a BackData Entry (BDE) mode that allows the information that clinicians wrote down in the filesto be entered into the application. BDE operates with somewhat looser validations andrestrictions. The reasoning behind this lowering of restrictions is that if shit hasalready gone down, there is no point in correcting it at that point. If a patient hasbeen given unsuitable medication, that incident needs to be kept track of, as it is nowthe patient’s history. At the time I left all this ART business in 2021, there were talksof introducing Optical Character Recognition (OCR) for BDE. I don’t know how far the ARTteam has gone with this but we can all agree this is awesome stuff.What’s the point I am trying to make?As developers we must realise that we are developing applications for low resourceenvironments. It’s not out of the ordinary to have 18 hours of no electricity in Malawi.Almost every week we get an SMS from one of our ISPs that fibre waduka ku Mazambiki.In addition to all of this, we must be aware that we will never stop developing buggyapplications (even with this AI powered programming). Develop applications with a mindsetthat they will fail and have a back up plan for when that happens. In my opinion, I believephysical files are one of the best tools we have available for our back up plans. Embracefile based workflows… I die a little every time I fail to access some service becausepalibe network. A service that can technically be provided bhobho without a computer isblocked chifukwa network palibe? Pathetic, really… This is a problem that some Malawianssolved cha m’ma 2005 mu. Of course outsourcing development of the applications used inthe provision of these services does not help at all. Local developers have more contexton Malawi’s challenges than anthu akunja." }, { "title": "I Hate File System Based Routing", "url": "/posts/I-Hate-File-System-Based-Routing/", "categories": "Programming", "tags": "programming, javascript, sveltekit", "date": "2023-07-30 23:00:00 +0200", "snippet": "I have been using Sveltekit on a number of projects for a while now. Whilst I like someof its ideas, there is one thing that just undoes the whole thing for me. File systembased routing, it sounds ...", "content": "I have been using Sveltekit on a number of projects for a while now. Whilst I like someof its ideas, there is one thing that just undoes the whole thing for me. File systembased routing, it sounds like a good idea but in practice it’s annoying as f%$k on anymoderately complex application. It pretty much breaks how I work with any of my texteditors of choice (neovim / vscode).First problem, tabs are rendered less useful. When you have 10 files open that are allnamed +page.svelte in tabs, it becomes harder to know which tab to go to for aspecific file. Yes, some text editors include some information about the directorycontaining the file but it’s still a long way from improving the user experience.In the absence of this file system routing nonsense, finding the file with the productedit logic is as simple as just scanning for for a tab with a label likeEditProduct.svelte, not trying to make sense of labels that look somethinglike +page.svelte (../[id]).Secondly and the biggest deal breaker for me, it breaks fuzzy file search. Jumping betweenfiles in a text editor for me involves using a fuzzy file search tool like FZF or vscode’s** thingy. I type the name of the file I want and hit enter to jump to it.I find that workflow super fast, I have been using it for years. I don't need to reach fora mouse and start clicking through menus and what not. No need to click through some filesystem tree view to search for some file. With file system based routing, fuzzy findingfiles becomes 10 times harder. Searches become less precise since you mostly matchdirectories containing a shit ton of files and directories. Fuzzy finding files becomessearch for what you want then scroll through a list containing a bunch of files you arenot interested in. I am having to unlearn how I use a text editor for the sake ofSveltekit.A work around for these problems involves pushing as much logic out of Sveltekit’sroutes folder to a component hierarchy outside. Basically, your +page.svelte justimports a component from elsewhere containing all the logic. The component’s file canhave a nice name that’s easy to remember (e.g. pages/EditProduct.svelte). However, bydesign Sveltekit pulls you to have a good amount of pages’ logic in your routes directory.The logic needed to do stuff in your component is normally spread out in a number offiles (+page.svelte, +page.js, +page.server.js, +layout.server.js, etc).Ideally, you don’t want your logic spread all over so naturally you gravite towards havinga big part of pages’ logic in +page.svelte. Frameworks are supposed to make work easier.In this case, I am needing to find work arounds in order to make working with theframework easier. In my opinion, this is a sure sign that something is awfully wrong." }, { "title": "FAM On That Crystal Meth", "url": "/posts/FAM-On-That-Crystal-Meth/", "categories": "Programming", "tags": "crystal, c, linux", "date": "2023-07-27 20:00:00 +0200", "snippet": "One thing that got me into Crystal is how easy creating Cbindings is in the language. Here is a quick example:$ crystal init app libc_bindings$ cd libc_bindings$ cat &lt;&lt;-EOF &gt; src/libstdio....", "content": "One thing that got me into Crystal is how easy creating Cbindings is in the language. Here is a quick example:$ crystal init app libc_bindings$ cd libc_bindings$ cat &lt;&lt;-EOF &gt; src/libstdio.crlib LibStdio fun puts(Pointer(LibC::Char)) : LibC::IntendEOF$ cat &lt;&lt;- EOF &gt; src/libc_bindings.crrequire \"./libstdio\"module LibcBindings subject = \"world\" LibStdio.puts(\"Hello #{subject}\")endEOF$ crystal build src/libc_bindings.cr$ ./libc_bindings=&gt; Hello worldSimple, right? In case, this went over your head, this is what is going on: Initialised a new Crystal application with the name libc_bindings Created a file that contains a definition of the puts function from libc(I have omitted some linker flags that are supposed to be passed to the libdeclaration as an annotation because Crystal links to libc by default) Created a file containing your every day Crystal code that calls the puts function Build application Run itWhat’s with this FAM business thenSome 7+ months ago, I was trying to create a binding forinotify(7) in Crystal andI run into an area that’s somewhat undocumented in Crystal. So, I needed to bind to thefollowing C struct:struct inotify_event { int wd; uint32_t mask; uint32_t cookie; uint32_t len; char name[];};Notice that the last field in the struct is a Flexible Array Member (FAM). Let’s takea short detour for those that don’t know what FAM is. Normally in C, structs havea defined size at compile time. So if you need to embed a string or something else witha size that’s not known at compile time in a struct, you define a field with an openended size (can only be the last member of the struct). When allocating memory for thestruct you add the size of the string you want to include in the struct:int main() { char *name = getStringFromSomewhere(); struct inotify_event *event_p = malloc(sizeof(inotify_event) + strlen(name) + 1); // Don't forget the \\0 ...}FAMs are quite easy to wrap your head around in C (although their use in code is 9/10times questionable) but when it comes to other languages that bind to C, they are a painand usually not supported at all (Golang doesn’t in case you wondering). Crystal happensto not have first class support for these pieces of shit too. I initially gave up oncreating the notify(7) bindings after running into this block. Later on though, aftermizimu yakumidima itandinong’oneza I came back to it. It was painful to get it workinggiven that my knowledge of C bindings in Crystal was mostly surface level (still is) butI survived. Here is how I went about it…I first had to define the struct like this:lib Inotify struct Inotify_Event # Crystal converts the name above to inotify_event wd : LibC::Int mask : UInt32 cookie : UInt32 len : UInt32 name : LibC::Char end type Event = Inotify_Event # Alias the name to something nicerendNotice that I have defined the FAM field name as a char. So the size of this objectin Crystal would be sizeof(struct inotify_event) + sizeof(char). If I try to accessthe name (assuming name is allocated), I will get back the first letter of thename. That isn’t very useful, however we can use the address of the name field to readthe entire string out. I ended up creating a helper function to extract the name.module LibInotifyHelpers def self.extract_name(event_p : Pointer(LibInotify::Event)) : Pointer(LibC::Char) offset = offsetof(LibInotify::Inotify_Event, @name) address = event_p.address + offset Pointer(LibC::Char).new(address) endendThe function first of all gets the offset of the name field (@name) intostruct inotify_event. This offset is then added to the address of the struct inmemory coming up a with a new address. The new address is used to initialise achar *. This effectively gives us a string. Here is how I ended up using this all:def watch loop do event_p = Pointer .malloc(sizeof(LibInotify::Event) + FILENAME_MAX_LENGTH + 1, UInt8) .as(Pointer(LibInotify::Event)) status = LibInotify.read(@file_descriptor, event_p, sizeof(LibInotify::Event) + FILENAME_MAX_LENGTH + 1) raise \"Failed to read inotify event: errno(#{Errno.value})\" if status.negative? event_type = parse_event_type(event_p.value.mask) filename = LibInotifyHelpers.extract_name(event_p) yield event_type, String.new(filename) endendCool stuff, I know… but highly unsafe… Good for educational purposes only in myopinion." }, { "title": "A Naive Approach to Monitoring Datasets [Draft]", "url": "/posts/A-naive-approach-to-monitoring-datasets/", "categories": "Programming", "tags": "python, programming, data engineering", "date": "2023-05-23 18:00:00 +0200", "snippet": "For some time now, I have had this problem at my usual place of work to do with monitoringof some data streams. The feeds are mostly set up as webhooks that are provided to otherteams within the or...", "content": "For some time now, I have had this problem at my usual place of work to do with monitoringof some data streams. The feeds are mostly set up as webhooks that are provided to otherteams within the organisation I work at and/or external organisations to send various datathrough. The data ranges from notifications from mobile money partners to events fromvarious applications in use within the organisation. This data is dumped with some slightmodifications into our object storage solution (we made a poor decisionin the choice of our object storage solution – believe it or not it is kind ofopinionated about the structure of the data dumped into it… Anyway, story for anotherday). I was very much interested in the patterns of flow of the data. I wanted to bealerted if we start getting an unusually low or high inflow of data. Unusual inflows ofdata can sometimes indicate a problem that needs to be addressed ASAP before shit goestoo far south.I scoured the Internet for possible solutions I could use for this. Turns out 99% of thegood ones require a good amount of data science knowledge. I know nothing about datascience so I really couldn’t use any of them. The remaining 1% were extremely naivesolutions that really wouldn’t work well for me. Examples of the extremely naive solutionsinclude simply checking for flatlines in the data (i.e. we haven’t had any data in the past24 hours). Checking for flatlines works for datasets that are expected to get data everyday but I have some datasets that can go for a couple of days without any data. Besides,this was already something that was implemented on some of the datasets but behold therewere still issues we were missing. Flatlines alone are not enough. Like I have mentionedelsewhere in this article, I was also interested in extremely high inflows of data.Eventually, I settled on two solutions… They are quite naive as well but I felt theywere quite better than the rest of the other solutions and would complement the existingmonitoring solutions quite well.Difference of MeansThe first solutions was the good olddifference of means t-test.I did not run into anything that explictly mentioned this as a solution. I kind of inferredit from the material I was reading… And to be honest I had already made the decision totry this out even before I did any kind of research on this. I figured this could be usedto compare the current average inflow of data to the past average inflow of data ona particular stream. There are some (a lot of) underlying assumptions I was making aboutthe data but what the hell, right??? Better to try something dumb than nothing at all.Here is how I staged it all… I gather data from the last 30 days on number of new rows added to a dataset daily I split the data into two groups Recent data (i.e. from the past 7 days) Old data (i.e. everything before the last 7 days) Compare the means from the two groups to see if there is a significant differencebetween themWhy did I pick 30 days… Well I figured that over long periods of time, the inflows ofdata do change. In the beginning getting 100 new rows per day was the standard, 3 monthslater the standard is now 10,000 rows per day. Trying to include all these data pointswould result in very large standard deviations making almost everything acceptable. Overa month the variations would be somewhat manageable.Isn’t 30 (ie… 22 data points vs 7 data points) too small for a sample? That was myfear but again, no one’s life depends on this. So, I winged it hoping for a good resultand I was handsomely rewarded for it in the end.How does this look in code???from datetime import date, timedeltafrom pandas import Datasettoday = date.today()start_date = today - datetime.timedelta(days=30)dataset_name = \"Sexy ass dataset\"inflows : Dataset = get_daily_inflows_for_dataset(dataset_name, from=start_date)current = inflows[inflows[\"Date\"] &gt;= date]historical = inflows[inflows[\"Date\"] &lt; date]import statst_stat, p_value = stats.ttest_ind(historical[\"Count\"], current[\"Count\"]# Using 5% significance levelif p_value &lt;= 0.05: send_alert(dataset_name, {\"t-stat\": t_stat, \"p-value\": p_value})TBC" }, { "title": "Please Format your Squeel!!!", "url": "/posts/please-format-your-squeel/", "categories": "Programming", "tags": "sequel, sql", "date": "2023-04-30 21:00:00 +0200", "snippet": "This might be short… Mostly a rant on the lack of etiquette most developershave when it comes to writing SQL.There are two things that irritate me without end when it comes to readingother peoples ...", "content": "This might be short… Mostly a rant on the lack of etiquette most developershave when it comes to writing SQL.There are two things that irritate me without end when it comes to readingother peoples SQL: Abuse of ALIAS Omitting of a field’s source table in the SELECTed fields Lack of or inconsistent formatting of code1. Abuse of ALIASWhat do I mean by abuse of ALIAS? The majority of developers tend to useALIAS to shorten names of database objects to one letter names. I do notfully understand why they do this but as far as I can tell, they do thisto save a few keystrokes. For example for a table named people, someefficiency expert will alias the table name to p to save himself thetrouble of hitting 5 letters on his keyboard. I do sort of understand thisreasoning, however I think the cost of doing this far outweighs thepercieved efficiency gains.By doing this crazy aliasing you make the query harder to understand. Let’slook at an example to help me make my point:SELECT p.name, p.dob, p.gender, a.city, a.village, a.districtFROM people AS pLEFT JOIN address AS a ON a.id = p.person_idWhen reading the query above, you start from the top. You look at the SELECTfollowed by the list of fields being selected and so on. Now the first timeyou get to the list of selected fields, you will not be able to make anysense of what you are seeing without having any prior information. In thequery what exactly is p.name supposed to be? A name of a pharmacy or a nameof pants??? Of course, from the above you can guess that this is a name ofperson but still my point stands. For you to make sense of what p is,you must scan ahead to the FROM part of the query to see what p is.From that point on you need to maintain in your head that p = people.If there are a couple of more tables being joined to, then you have tomaintain a mapping of all those aliases and must continuously substitutethose aliases throughout the query. The query in itself might be complex andfor some super self serving reason you decide to add unnecessary complexityto the query.Imagine if we did this in every program we write (granted Go developersdo it, but those guys are nuts). Every variable is aliased to some shortone letter name. Reading code would become such a laborious task. We canagree that most of us enjoy writing code not reading it. So let’s try toavoid making an already boring task even more painful than it already is.So where do I think aliases should be used?I think aliases should be used to add more context to a queryor remove unnecessary context.Adding more context to a query… Let’s look at the following query:SELECT employees.name, employees.dob, employees.genderFROM people AS employeesWHERE employees.type = 'Employee'In the above query, I am trying to select all people that are employees.So by aliasing people to employees, I am effectively adding a bit morecontext to every reference I have of people in my query. A reader ofthe query sees employees and they understand what that is immediately,without needing to read ahead and track back.Another reason for aliasing is to remove unnecessary context. On thisreason, I am kind of on the fences but there are places where I thinkit doesn’t really do any harm. Consider the following query:SELECT people.name, people.dob, people.genderFROM human_resource__people AS peopleIn the query above, there is this funny namespacing of tablesthat’s going on. If you have used something like Django then you haveprobably run into this. Basically a Django project is an umbrellaproject to which you add a bunch of small applications (kind of boundedcontexts in Domain Driven Design). Each application is namespaced in thedatabase. So multiple applications can have a domain model with the same name.Now in the query above we are simply interested in people from thehuman_resources context. Continuously repeating that fact in the querydoes not really add much value. If you are looking at that query inan application then that fact must be implicit in the name given to thatquery.2. Omitting of a field’s source in SELECT blocksIt’s quite common for people to omit table names when referencing fieldnames in a query. An example:SELECT name, dob, gender, city, village, districtFROM peopleLEFT JOIN address ON address.id = people.person_idWHERE primary = TRUEThe query above is selecting fields from two different tables. You can’tsimply look at that query and easily tell where any of the fields arecoming from. Is city from people or from address? And to make matters worsethere is a field called primary that is being reference in the WHERE partof the query. Where the hell is that field coming from? Need I say more?My general recommendation on this is to never omit the source table nameswhenever a query directly references two or more tables. If you havesubqueries in conditions and that kind of stuff, this really doesn’t apply.The only time you are free to omit the table names is when you only havea FROM that references one table without any JOINs. The context in suchcases is unambiguous, no need to remind me of it.3. Lack of or inconsistent formattingThis one bugs me a lot… Everytime I see it I tend to question the sanityof the person that was writing the query. It’s a clear lack of organisationthat can only come from being brain damaged. Let’s start with an example:selectpeople.name,people.dob,people.gender, address.city, address.village,address.district from people left join address on address.id = person.person_idleft join identifierson identifiers.person_id = people.idwhere people.type = 'Employee'Reading code generally relies on visual cues. It’s rare for anyone that hasbeen programming for a while to read code line by line. You do go lineby line when you want to zone in on a particular section of the code butfor the rest it’s mostly scanning of the code and quickly identifying patternsin the code (this is why idioms matter in programming). Going to the queryabove it’s very difficult to identify different sections of the query asit is. Say for example you want to add a new JOIN to the query, you can’tsimply jump to a specific part of the query and make the change. You needto read every single line of that query to identify where you need to makethe change (granted you can just search for join and add your thing rightbefore it). Combine this with the other two problems mentioned in thisarticle, you have a serious problem on your hands.Now let’s format the query and see how it improves things:SELECT people.name, people.dob, people.gender, address.city, address.village, address.districtFROM peopleLEFT JOIN address ON address.id = person.person_idLEFT JOIN identifiers ON identifiers.person_id = people.idWHERE people.type = 'Employee'Better, isn’t it? You can easily identify different blocks in the code.Even if you aren’t familiar with this style, you can easily jump toa specific part of the code and make changes. That’s the style I usebut you are free to go with one that makes the most sense to you ratherthan nothing at all.A bit of an explanation of the style I go with: Use indentation to indicate that a block directly supports thestatement above. The block is more of a qualifier for the statement above.So if you aren’t interested in the statement above, you can happily ignorethe entire indented block that follows it. Use CamelCase or snake_case for identifiers Uppercase all SQL keywords to make them stand out from identifiers In JOIN conditions always lead with the identifiers from the table beingjoined to. That is:LEFT JOIN address address.value = some_other_table.value-- NotLEFT JOIN address some_other_table.value = address.valueI have more rules that I follow in my queries but these are the primaryones and they help me easily make sense of my queries. As a bonus,most times when I deviate from this style it indicates to me that I amdoing something wrong." }, { "title": "Docker Network Interface Woes", "url": "/posts/Docker-Network-Interface-Woes/", "categories": "System administration", "tags": "docker, firewalld, linux, system administration", "date": "2022-12-23 11:30:00 +0200", "snippet": "After some system updates on my Opensuse Tumbleweed box, dockerd could notstart. Any attempts to start it would be responded to with the followingerror:WARN[2022-12-23T13:17:02.715976321+02:00] cou...", "content": "After some system updates on my Opensuse Tumbleweed box, dockerd could notstart. Any attempts to start it would be responded to with the followingerror:WARN[2022-12-23T13:17:02.715976321+02:00] could not create bridge network for id befcb314f3a719effeaf8ab7910123a5c20528603e20d437a42163583523b898 bridge name docker0 while booting up from persistent state: Failed to program NAT chain: ZONE_CONFLICT: 'docker0' already bound to 'internal'failed to start daemon: Error initializing network controller: Error creating default \"bridge\" network: Failed to program NAT chain: ZONE_CONFLICT: 'docker0' already bound to 'internal'My first thought was to maybe delete the docker0 network interface.So, I went ahead and deleted it using:sudo ip link delete docker0Unfortunately, this didn’t resolve the issue. A bit of Googling led me tounderstand that I was looking at the wrong place. The problem was on thefirewall configuration. To verify this, I had to run the following:$ firewall-cmd --get-active-zonesdocker interfaces: br-7a20169cf60ainternal interfaces: docker0public interfaces: enp0s20f0u1From the output of the command above, the source of the error becomes clear.The old docker daemon was using the docker0 interface under the internalzone, whilst the new docker daemon is trying to create a new docker0 interfaceunder the docker zone. I tried a couple of solutions before I landed on thecorrect one.The following is what I tried and didn’t work:$ sudo firewall-cmd --permanent --zone=internal --remove-interface=docker0$ sudo firewall-cmd --reload# The above didn't work$ sudo firewall-cmd --permanent --zone=docker --change-interface=docker0$ sudo firewall-cmd --reload# This too, didn't workClose to giving up, I run intothis gist.I noted that the author was pretty much doing the same as I was doingwith one exception, they weren’t simply reloading the firewall rules. Theywere doing a complete restart. So, I tried that and it worked. If you runinto this issue again Mr Ntumbuka, please try this first and if it doesn’twork, go ahead and repeat everything above:$ sudo firewall-cmd --permanent --zone=docker --change-interface=docker0$ sudo systemctl restart firewalld" }, { "title": "How to Wait for Dependencies in Docker Compose", "url": "/posts/How-to-properly-wait-for-your-dependencies-in-docker-compose/", "categories": "Programming", "tags": "docker, linux", "date": "2022-12-18 22:30:00 +0200", "snippet": "Every once in a while you may find yourself having to deal with servicesthat have some hard dependencies on each other. Service B (frontend) needsto have service A (API) running before it starts. I...", "content": "Every once in a while you may find yourself having to deal with servicesthat have some hard dependencies on each other. Service B (frontend) needsto have service A (API) running before it starts. In most cases the bestway to deal with this sort of thing is to just restart service B aftera couple of probes for responsiveness. The service’s exit code can alsobe used for this purpose. Say, these solutions don’t work all that wellfor you and you really need to have your services wait for each other,here is something you can do.Let’s work with a theoretical Django application. You need to runmigrations run when the database service is ready. First let’s comeup with a Dockerfile for our application:FROM python:3.10RUN apt-get update -qq# wget is required for probing HTTP and netcat for raw TCPRUN apt-get install -y wget netcatRUN curl -L -o /usr/local/bin/wait-for https://raw.githubusercontent.com/eficode/wait-for/v2.2.3/wait-forRUN chmod +x /usr/local/bin/wait-forWORKDIR /opt/django_app/COPY . /opt/django_app/RUN pip install poetryRUN poetry config virtualenvs.create falseRUN poetry install $(test \"$ENV\" == production &amp;&amp; echo \"--no-dev\") --no-interaction --no-ansiCMD [\"python\", \"manage\"]In the Dockerfile above, we are installing a tool,wait-for, that polls an HTTP orTCP service and runs a command when the service is available (TCP) orresponding with a 200 - OK (for HTTP). The tool depends on wgetand netcat. If you are only interested in an HTTP service then you onlyneed to install wget, else you need netcat. Next thing, let’s defineour docker-compose.yml.version: \"3.9\"x-pg-env: &amp;pg-env POSTGRES_USER: ${POSTGRES_USER:-iamgroot} POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-iamgroot} POSTGRES_DB: ${POSTGRES_DB:-groot}services: app: build: . command: python manage.py runserver ports: - 8000:8000 environment: &lt;&lt;: *pg-env depends_on: - database links: - database restart: always db-migrations: build: . command: sh -c 'wait-for database:5432 --timeout 10 -- python manage.py migrate' environment: &lt;&lt;: *pg-env depends_on: - database links: - database restart: on-failure database: image: postgres environment: &lt;&lt;: *pg-env ports: - 5432:5432 restart: alwaysFrom the compose file above, zone in on the db-migrations service.We have defined a command that first of all tries to wait for postgresto start. It waits for 10 seconds and fails if not successful withinthat time. Otherwise, if a connection is made within that 10 seconds,the command python manage.py migrate is executed. The above is nota very good use case for this, because as you can see, ifdb-migrations fails then it will get executed again. I have howeverrun into situations were I had no option but to do something likethis (e.g. start a frontend service only when the backend serviceis running - frontend built with an implicit assumption that the backendwill never be unavailable. It happens more often than you would think.When is the last time your frontend handled a network error not justa status code?).Happy holidays!!!" }, { "title": "Careful With Them Elasticsearch Ingest Pipelines", "url": "/posts/Careful-With-Elasticsearch-Ingest-Pipelines/", "categories": "Databases", "tags": "databases, elasticsearch, couchbase", "date": "2022-12-14 19:40:00 +0200", "snippet": "About a week ago I run into some mind boggling problem workingwith Elasticsearch.Looking back at it now, it seems quite obvious but back then it wasnot at all straight forward to figure out.The pro...", "content": "About a week ago I run into some mind boggling problem workingwith Elasticsearch.Looking back at it now, it seems quite obvious but back then it wasnot at all straight forward to figure out.The problem stemmed from this: I have data inCouchbase that I need to getreplicated to Elasticsearch but unfortunately for me that data has somefields like _id that Elasticsearch does not accept. So for a quick fix,I decided to throw in anElasticsearch Ingest Pipelinewhich would remove the offending fields as the documents are gettingwritten to Elasticsearch. So I set up myreplication agentwith the ingest pipeline in place. A quick glance at the data led me tobelieve that everything was okay. All the data had been replicatedand all new records coming from Couchbase were getting created inElasticsearch, LG. Time passes, and I realise that I havesignificantly more documents in Elasticsearch than I have in Couchbase.That’s when all the fun began. I was completely lost trying to figureout the cause of this disparity.I sat down and looked deep into the data I had. First thing Idiscovered was that the document _ids in Elasticsearchwere not matching thedocument keysin Couchbase. To be frank, I really didn’t know that they are supposedto match, I just had a hunch. Secondly, I had duplicate documents.On closer inspection of the duplicate documents, I discovered thatthese documents although the same had some fields that were different.Essentially, there was one original document and a number of documentsfollowing that, that were just updates.At this point it was becoming clear that the source of the problem was thedocument key from Couchbase. Somehow Couchbase was not passing thedocument key to Elasticsearch, hence Elasticsearch was not doing anyupdates when it received an already existing document. There is no _idspecified thus Elasticsearch just creates a new record. Underneath, theCouchbase - Elasticsearch replicator was sending bulk requests toElasticsearch. Those look something like this:POST /_bulk?pipeline=&lt;ingest-pipeline-name&gt;{\"index\": {\"_id\": \"&lt;document-key&gt;\", \"_index\": \"&lt;index-name&gt;\"}}{ \"_id\": \"some-value\", \"field1\": \"value\", ... \"fieldN\": \"value\"}{\"index\": {\"_id\": \"&lt;another-document-key&gt;\", \"_index\": \"&lt;index-name&gt;\"}}{ \"_id\": \"some-value\", \"field1\": \"value\", ... \"fieldN\": \"value\"}The ingest pipeline I had in place, was targeting the _id inthe body of the document and not the _id in the index command(that’s what I thought at least). If the _id in index commandis missing, that’s when Elasticsearch creates a new document.Went on a wild goose chase through out thecouchbase - elasticsearch connector code trying to find ifthere was any chance of the document key getting omitted whenbuilding the bulk request sent to Elasticsearch. Found nothingof that sort, it was quite obvious the connector was not messingup at all (points for Open Source Software - if the connectorwas proprietary I would have probably convinced myself that theconnector was the problem).Following the Sherlock Holmesian fallacy “when you have eliminatedthe impossible, whatever remains, however improbable, must be thetruth”, I came to the conclusion that the Elasticsearch IngestPipeline was the problem. So, I set up some test documents inCouchbase with no _id field and started replicatingthem to Elasticsearch, with and without the Ingest Pipelinein place. Sure enough, the Ingest Pipeline turned out to be theproblem. Lucky for me, the owners of the Couchbase data were okaywith removing the offending _id field. The field was a remnant of thepast that was no longer in use. Removing it would not result in death.Happy ending in the end for all of us.Still, I had one thing lingering on my mind that was unresolved. Whywas the Ingest Pipeline not working correctly? Turns out, theElasticsearch teamhave made it possible to access a document’s metadata fieldsas part of the document itself within an Ingest Pipeline. For a fieldnamed foobar, I can access it by just referring to foobar inan Ingest Pipeline. With this feature, I can do the same with metadatafields like _id. So, when the data was coming into the IngestPipeline, the _id I had as part of the document, was gettingoverriden by the document key I had as part of the index request.In addition, modifying these metadata does affect the document’sactual metadata. So if I change an _index field’s value, thatwould effectively change the index the document is written to.Strangely, although I had access to the source document throughthe _source field in the Ingest Pipeline, modifying the_id through the source document had a net effect that was exactlythe same as modifying the global _id (this looks to me like a bugin the Ingest Pipeline - I should at the very least be able toaccess the raw source document through _source).Moral of the story, I think that would beRead The Whole Fucking Manual (RTWFM). But even if I did, I am sureI still would have had trouble figuring out what was going on.I would have simply done something likerename _source._id to something_else which still wouldn’t haveworked. There is no easy work around for this as far as I know.ASIDE: Writing this has made me realise that having a no-op IngestPipeline would have worked. I am not sure if Elasticsearch allows thisthough. Think about it…" }, { "title": "Crystal Has Java-like Interfaces", "url": "/posts/Crystal-has-Java-like-interfaces/", "categories": "Programming", "tags": "crystal, oop", "date": "2022-11-14 21:46:00 +0200", "snippet": "I love Crystal; I think it gets right pretty much everything that irks meabout Ruby. And that speaks volumes because Ruby to me is just brilliant butit’s not something I would happily use in a larg...", "content": "I love Crystal; I think it gets right pretty much everything that irks meabout Ruby. And that speaks volumes because Ruby to me is just brilliant butit’s not something I would happily use in a large project. Enough about that,here is what I want to keep on record here. Crystal is damn near a completelanguage to me but I have always felt that having interfaces as in Go or Javawould certainly make certain things nicer. If you go through the officialCrystal guides like I did, you will be convinced that Crystal does not haveinterfaces. To my surprise I just learnt (a couple of minutes ago) that Crystaldoes kind of have interfaces and they work in the Java sense of interface.I learnt about this through atwo year old threadon the Crystal forum where someone was asking for interfaces in Crystal likewe have elsewhere in the world of OOP. Two years later the thread got aresponse and here is what I have learnt: Crystal doesn’t have an explicit interface keyword (already knew this) Crystal has abstract classes and methods (knew this too) Modules can have abstract methods (oh shit - I have never even considered that) When modules with abstract methods are included in a class, the class mustprovide an implementation of the abstract methods When a class includes a module, the class satisfies the module’s interface.In other words the class can be used where ever the module is expected (Ido recall trying something like but not working out for me, let’s see…)Here is how you could go about defining an interface in Crystal:# Define the interface as a module with abstract methodsmodule Foo abstract def name : Stringend# Define a class and include the module/interfaceclass FooBar include Foo # Shades of Java's implements, eh??? getter name : String def initialize(@name); endend# Define methods that consumes or produces the interface/moduledef print_foo_name(foo : Foo) puts foo.nameendfoobar = FooBar.new(\"J.Random\")print_foo_name(foobar) # prints \"J.Random\"There you have it… Interfaces in Crystal!!!" }, { "title": "Of Essence And Accidents in Software Development", "url": "/posts/Of-Essence-And-Accidents-in-Software-Development/", "categories": "Programming", "tags": "programming, software development, software architecture", "date": "2022-08-30 20:47:00 +0200", "snippet": "A couple of days ago I run into a meme on Fred Brooks’, No Silver Bullet.I don’t remember what exactly the meme was about or where I saw it, justthat it had something to do with this awesome paper....", "content": "A couple of days ago I run into a meme on Fred Brooks’, No Silver Bullet.I don’t remember what exactly the meme was about or where I saw it, justthat it had something to do with this awesome paper. It happens to be oneof my favourite papers of all time in computing. The paper talks aboutsomething that most of us software developers think and claim is obviousbut somehow we frequently ignore.The biggest take away for me from that essay is that there are two mainsources of complexity in software. There is complexity that comes fromthe inherent difficulty in the problem we are solving. This is calledessential complexity (complexity that emerges from the essence of theproblem we are trying to solve). There isn’t too much that can be doneto get around this. Your solution regardless of what you do will tendtowards complexity because of it.The other source of complexity happens to be around the choices wemake. Choices range from the tools we chose, all the way down to thetiniest architectural choices. This is essentially self inflictedpain and it is referred to as accidental complexity. You are tryingto build a simple CLI app and you decide to go with Java (could beworse, PHP) because well, that’s all you know. Your choice of languagethere will be your main source of headaches.Accidental complexity is an odd phenomenon. For something that I amsupposed to be in control of, it has been the main source of pain inmy career as a software developer (or programmer). There have been caseswhere I have faced a problem that is indeed complex at its core but ifI am honest with myself, these are rare. For the most part, it is justme fighting the tool that I am using to solve a particular problem.ETBJ!!! Had this article stuck in draft for 3 months and completelyforgot about it… Publishing it as is." }, { "title": "Of Streams (Enumerators and Generators) in Ruby and Python", "url": "/posts/Of-streams-in-ruby-and-python/", "categories": "Programming", "tags": "programming, data engineering, python, ruby", "date": "2022-08-20 10:00:00 +0200", "snippet": "In a past life I was once tasked with coming up with a tool for extractingand transforming datasets of various sizes into datasets in a differentformat. The source datasets could hit sizes of up to...", "content": "In a past life I was once tasked with coming up with a tool for extractingand transforming datasets of various sizes into datasets in a differentformat. The source datasets could hit sizes of up to a couple of Gigabytesand this tool would have to be used in environments where memorywas quite limited. At the time, data engineering was not something thatI nor the rest of the team I was working with were into. And to make mattersworse, it would have been difficult to introduce a new set of tools andhave them deployed in remote areas with no internet connectivity. So forthe task, I ended up using our language of choice, Ruby. Ruby was available on all the machines this tool was to be run. Thesemachines were shipped to these remote areas with Ruby preinstalled.The tool was quite straightforward to build but the initial implementationwas somewhat naive. It loaded the entire dataset in memory and thenprocessed it from there on. This worked for the smallest of datasets butit obviously did not scale well especially in an environment wherememory came at a premium. So I altered the part of the program that wasresponsible for extracting the source datasets to paginate the data.At the same time I managed to keep the rest of the program unchanged.I did this by utilising Ruby’sEnumerator as follows:def load_dataset Enumerator.new do |enum| (0..).each do |partition| dataset = fetch_dataset_partition(partition, size: PARTITION_SIZE) dateset.each { |document| enum.yield(document) } break if dataset.size &lt; PARTITION_SIZE end endenddef process_dataset(dataset) dataset.each do |document| transformed_document = transform_document(document) save_document(transformed_document) endend Code has been heavily simplified for easy presentationThe Enumerator allowed me to break the source dataset into multiplesmaller chunks but maintain the illusion of having a single data streamto the rest of program. I successfully managed to significantly cut downon memory usage but still maintain the rest of the code base as is.Recently, I found myself faced with a similar issue but within the contextof a different programming language, Python. In thiscase, I was looking at a dataset that was partitioned at source. However,the output dataset had to be one (i.e. not partitioned). There were alsosome memory constraints but they were not as stringent as what I facedin the Ruby days (I can always negotiate for a bigger server).Each partition was reasonably large but not too big as to be impossibleto have all in memory. On the other hand, I could not have all thepartitions in memory at the same time because well … memory. Thepartitions all together are just too big to hold in memory. Faced withthis, I went back to that familiar pattern from Ruby and implemented itin Python. Now, Python does not have an Enumerator class like Ruby doesbut it has something else under the namegenerators. Generators are not to be confused with generator expressions. Theyare not the same thing.Generators in Python are simply functions that return a lazy iteratorto a stream of indefinite size. Python provides some simple syntaxthat allows one to generate a generator. Instead of using the returnstatement, one uses the yield statement in a manner similar toEnumerator#yield in the Ruby code above. For the problem I wasworking on, I ended up with code that looks something like thefollowing:def load_dataset() -&gt; Iterable[Any]: for partition in get_partitions(): dataset = fetch_dataset_partition(partition) for document in dataset: yield documentdef process_dataset(dataset: Iterable[Any]): dataframe: dict[str, list[Any]] = {'field_a': [], 'field_b': []} for i, document in enumerate(dataset): if (i + 1) % MAX_DATAFRAME_SIZE == 0: flush_dataframe(dataframe) dataframe_buffer = {'field_a': [], 'field_b': []} row = transform_document(document) append_to_dataframe(dataframe, row) flush_dataframe(dataframe) Again the code above is heavily simplified to keep things simpleFrom the code above, notice that I have taken multiple partitions ofa dataset and combined them into a single dataset without needing toload all the partitions into memory. Another thing to note is that,with the exception of the extraction code the rest of the code isagnostic of the nature of the data source. It just sees a streamof data coming in and not really care whether underlying the streamis a single data partition or a billion of them.Parting wordsWhat I have shown here is just one way of handling this sort of thing.You can easily do this differently and have code that quite easy togrok. I am only providing this to maybe give a different perspectiveon how to handle this problem using a set of tools you maybe have neve considered to take a closer look at. In my case, I tend to use these “streaming-like” solutions when I am faced with: A large data source that would best be handled by paginating itto minimise memory usage A naturally partitioned data source that I want to treat as one A potentially infinite data source that I have to poll at someintervalZipitani!!!" }, { "title": "Why logger.debug() when you could just print()?", "url": "/posts/Why-logger-dot-debug-when-you-could-just-print/", "categories": "Programming", "tags": "programming", "date": "2022-08-16 20:50:00 +0200", "snippet": "This is going to be a short one… print() statements and logger.debug()statements achieve the same thing for the most part. They both print someoutput in your programs to stdout. Using a logger does...", "content": "This is going to be a short one… print() statements and logger.debug()statements achieve the same thing for the most part. They both print someoutput in your programs to stdout. Using a logger does offer someadvantages over using bare print() statements. With a logger you caneasily suppress some output based on a logging level and/or redirect the outputto a file instead of stdout, for example. This is all great but I wantto try and sell something else that’s often not emphasised on usinga logger.print() and logger.debug() may achieve the same end butusing each of those functions expresses some intent. A print() statementin my opinion tells the reader of a program that this is output that theprogram is built to produce. In other words, the output being produced atthat particular line, meets a functional requirement. A logging statementcoming from a logger on the other hand expresses that this is output thatis latent to the primary operations of this program. It is there not to meeta functional requirement, rather it is meant to spill out the guts of theprogram for diagnostic purposes. When I see a print() statement in code,I zone onto it to see what it is printing out because I assume that theoutput being produced by that statement is there to meet a functionalrequirement. On the other hand, when I see a line starting with logger,my brain happily ignores those lines as they are of no consequence.I may have focused on logging here but this idea does apply to a numberof other things in programming in general. An example, A while andfor statement both do achieve the same thing but for certain kindsof use cases, one may be better at expressing a programmer’s intentthan the other. Keep that in mind… It is also a good idea to pickup on various idioms in one’s programming languages. Idioms are verygood at expressing intent to other seasoned programmers. An idiom makesit easy for one to tell what a programmer’s intentions are withouthaving to read every single line and grok the sum total of those lines.PISS OUT!!!" }, { "title": "Securing Datahub on k8s", "url": "/posts/Securing-Datahub-on-k8s/", "categories": "Data Engineering", "tags": "data engineering, data discovery, data catalog, k8s, kubernetes, notes", "date": "2022-08-09 18:00:00 +0200", "snippet": "I found myself having to deploy Datahub ona kubernetes cluster. Deploying the application is quite straight forward usingHelm. I did run into a tiny issue that stopped me in my tracksfor a moment b...", "content": "I found myself having to deploy Datahub ona kubernetes cluster. Deploying the application is quite straight forward usingHelm. I did run into a tiny issue that stopped me in my tracksfor a moment being somewhat new to kubernetes and helm. Datahub comes with a defaultaccount that has full administrator access to the application. The applicationprovides various mechanisms for authentication (e.g. OIDC), however, this account canstill be accessed by bypassing whatever authentication is set by simply navigatingdirectly to the /login endpoint. Fortunately, this account can be overriden byproviding a properties file with a different passwordfrom the default password.Now, in kubernetes we can mountour custom properties file into a running pod. I had an idea of how to do this but I wasa bit lost on how I could do this with helm involved. My understanding of how to mounta volume to a pod revolved around including the volume information in the pod definition.Being that I am working with a helm chart, I figured that the pod definition wasactually within the helm chart so I had no direct access to it. Stupidly, I went aheadand started going through the helm chart’ssource code hoping to find something thatmay help. That’s when I realised that the wonderful datahub guys provided a way for usersof the chart to extend the volumes definition for the pod. Worse this information waskind of there in the chart’s documentation but because I didn’t thoroughly go throughthe effin-manual I missed it. Anyway, the datahub chart provides two configurationparameters that can be used for what I needed. So here is how I went about it: Create a user.props file with a new password for the root account. The file lookedsomething like the following: datahub=new-password Create a config map from the properties file above: kubectl create configmap datahub-volumes --from-file=user.props Add the volume mount to my helm values file using the parameters provided by the chart.I ended up with a values file that looks something like this: # values.yaml datahub-frontend: enabled: true extraVolumes: name: user-props-volume configmap: name: datahub-volumes extraVolumeMounts: - name: user-props-volume mountPath: /datahub-frontend/conf/user.props subPath: user.props Install datahub helm install datahub datahub/datahub --values values.yaml A bit of an explanation of what is going on here… Volumes can be created froma number of sources including CnfigMaps. So, I created a ConfigMap in step (2) thatcontains the user.props file that I need to mount. In step (3) I am using thedatahub-frontend.extraVolumes and datahub-frontend.extraVolumeMount parametersprovided by the datahub chart to add more volumes to one of datahub’s pods.Under extraVolumes I define the volume under name user-props-volume and specify thatthis volume is from a configmap named datahub-volumes. And in extraVolumeMounts,I am telling kubernetes to mount a file named user.props which exists in volumedatahub-volumes at path /datahub-frontend/conf/user.props. The subPathparameter specifies the name of the file I want to mount instead of mounting theentire volume as a directory." }, { "title": "Data Engineering", "url": "/posts/Data-Engineering/", "categories": "Data Engineering", "tags": "data, data engineering, ETL, streaming, backend", "date": "2022-08-08 00:00:00 +0200", "snippet": "I felt like writing something today but really had nothing interesting to write about.To kill the urge I decided to write something about my current day job (well I do notlike it but it is unfortun...", "content": "I felt like writing something today but really had nothing interesting to write about.To kill the urge I decided to write something about my current day job (well I do notlike it but it is unfortunately the only thing I think I smart enough to do). So, inthis article I am going to explain what I think data engineering is, maybe go on toexplain where it fits into software architecture, some of the techniques involved init, and whatever else comes to mind as I write.What is data engineering?Data engineering can be summarised as an activity that involves extraction of datafrom source systems and repackaging that data in such a way that makes it easy todo all sorts of analytics on it. As a software developer (I am assuming you are)you have probably built applications that did two things for your users. It solvedsome particular transactional problem for them and then provided a way for theusers to extract some useful information from the application through reports.Think of a stock management application, you have a transactional part to it thatis responsible for collecting information about what goes into inventory and maybeoverall facilitates the processes involved in moving items to and from theinventory. These are clearly two separate use cases and they are formally calledOnline Transactional Processing (OLTP)and Online Analytical Processsing (OLAP)respectively. When modelling the database for this stock management application youwill (unknowingly) optimise for one of these two use cases. For transactionalprocessing, you need your writes and reads to be fast. You do not want to keepyour users waiting for transactions to process. The transactions should at leastbe as fast as the users physically move goods to and from inventory. In addition,at this particular point in time users are not really interested in aggregateinformation such as how fast is product X getting restocked/depleted. However, at theend of the month the users might be interested in knowing how fast product Xis getting depleted/restocked so that they can come up with a decision on theoptimal time to restock the product. This is the analytical part and as you cansee, users are not looking for an immediate answer. You have about a month toprovide a response to this question (i.e. if you knew that the question wasgoing to be asked before hand). For a report, a user is willing to click abutton and wait for the system to process the data then provide an answerafter some reasonable amount of time (say 5 minutes…).The nature of the queries made to the database for these two use cases are quitedifferent. For the transactional processing, the query is more row oriented.Give me product X, its price and all other related data. On the other hand, forthe analytical processing you get queries that are more column oriented.For product X, I need the total amount that was spent in purchasing it. Thatquestion mostly focuses on aggregating a particular column over a numberof rows. Modelling data for optimal performance and ease of use for each of these twouse cases will lead you to two very different solutions. For efficienttransactions, you will likely end up with some highly normalised model thatmakes extraction of rows from a database quite easy. And for the analyticalwork, you will likely end up with something not very normalised that makescolumn access quite efficient. This separation might even lead all the wayto the choice of database engine (column-store vs row-store). Of course,for the simplest of applications, a single data model works. One feature ofhighly normalised databases is that they offer a high degree of querypattern flexibilitywhich allows you to torture the data models with any kind of question. However,what you get in flexibility is offset by a loss in either query performance ordatabase scalability. Inevitably at some point, as you get a lot of data orthe application gets significantly complex, you will need to have an additionaldata model to specifically address your analytical needs.Data engineering is a discipline that bridges the gap between the transactionalneeds of an application and the analytical needs of the organisation theapplication serves. Data engineers (this name is unfortunate, IMHO they shouldbe called data plumbers) are tasked with extracting the transactional dataand optionally combining with data from other sources to produce datasets thatcan be easily analysed.Approaches to data extraction and processingThere are two general approaches that are followed in capturing and processingof data from various source systems. The approaches are mostly informed by thenature of the analytical needs and to some extent the architecture of the sourcesystems. Are you interested in realtime analytics (i.e. you need changes in thesource system to be immediately be propagated to your analytical systems or maybeyour analytics will be done at a later time)? Maybe you are interested in somecombination of the two? Is your source system built around an event drivenmicroservices architecture? The answer to these questions from a data engineeringis either streaming or traditional Extract-Transform-Load (ETL) or some combinationof the two.StreamingIn streaming, you basically attach to event stream and you process the eventsas they come. Say, you are interested in reporting all error 500s that exceeda certain threshold in a given amount of time. I mean you could say, we wantto know everytime a billion error 500s occur within a 5 minute period. In thatcase you can attach to a log stream and check whether the total number of errorsthat have occured in the last 5 minutes have hit a billion. You can then savethat data as a new dataset or simply trigger some reporting jobs.There are a number of tools out there in the wild that allow you to do thesekinds of analytics. Of the top of my head I can think ofBenthos,Spark Streaming, andFlink. Some of the features I look for when choosinga streaming solution include windowing and sliding of those windows. A window isjust a period of time you want to focus on and a slide is all about how frequentlyyou need that window to be moved. For example, you could say I want to lookat data from the past 5 minutes (window) and I need this 5 minute window toprogress forward in time after 2 minutes (slide). So, what happens in thatcase, is that the tool in use will buffer data for 5 minutes and make thatdata available to you as a single dataset so that you can do your analyticalprocessing on it. These 5 minute windows can then be updated every two minuteswith new data (you essentially have a dataset with data ranging from about 2to 7 minutes ago).You were probably expecting to hear something to do withKafka. Well, in my experience Kafkamakes for a good starting point for your streaming solutions. You canthink of it as a persistent log for your data streams. If you havea continuous stream of data you would first have it write to Kafka thenyou stream processing starts off from Kafka. In as much as you can directlyattach to a service bus of your event driven application, you probably willwant to buffer this data somewhere (that is where Kafka comes in).Traditional ETLForgive me for using the word “Traditional” here. Nowadays, the linebetween ETL and streaming is kind of getting too blurred to warrant aseparate treatment of each of these techniques. Tools like Apache Flinkactually do handle both ETL and Streaming under the same framework.Working in Flink makes you think “streaming-ish” even of you are workingwith a data source that is not a stream. In ETL, you have a data sourcethat is not a stream for example an ordinary relational database.So, you pretty much grab all the available in source then process thatdata in a processing framework like Spark.You may join this data to other datasets from different sources and atthe end of all this you create new datasets.This kind of workflow runs on a schedule. It could be something like,every 3 hours grab all the data from the source system and thenprocess it somehow. Of course, you can include some techniques toonly capture and process the changes in the data. In general though,you will be performing an extract, transform, and load at a givenschedule. Tooling used for this include ordinary cron and somethingas sophisticated as AirflowOne of the main gotchas with ETL is that you could end up puttinga lot of stress on the source system with all the extraction work.Data extraction is a resource intensive process and could resultin significant slowing down of the source system as the extractionprocess hogs all resources. This can be mitigated to some extentby setting replication to a different database and doing the extractionfrom thereon. This does work to some extent but it is not alwaysthe best solution. To work around this, it is quite common to resortto reading the database transaction log directly and then processingthose logs in a style similar to stream processing. This reducesthe load on the source system and eliminates the need to perform anysort of magic to determine what has changed in the data since thelast extraction process. There is some tooling that has been builtaround this, examples include Debezium andMaxwell’s Daemon. I tend to separatethis from the other kind of ETL, thus I call the other kind ofETL just “traditional” ETL to differentiate it from this. This isstill ETL but with a slightly different approach that is extremelyclose to stream processing.Closing wordsI have so much to say on this topic but I will leave it here now.More will covered in other blog posts if I am up to it. Interestingtopics from here include data lakes, data warehouses, data lakehouses,star schemas, OLAP cubes, and many more. I hope to cover these someother day. What I have covered here should be enough to introducedata engineering I believe." }, { "title": "Ubuntu 22.04 Has Some Serious Issues", "url": "/posts/Ubuntu-22.04-Has-Some-Serious-Issues/", "categories": "GNU/Linux", "tags": "linux, gnu, ubuntu", "date": "2022-06-23 17:20:00 +0200", "snippet": "Ubuntu 22.04 has been out in the wild for a while now. Initial reviews of itwhere positive overall. A lot of people claimed that it was the best Ubunturelease in a very long time. So, I grabbed an ...", "content": "Ubuntu 22.04 has been out in the wild for a while now. Initial reviews of itwhere positive overall. A lot of people claimed that it was the best Ubunturelease in a very long time. So, I grabbed an ISO of the new Ubuntu and checkedit out. In all honesty, I didn’t see much of a difference from the lastLTS release. There were a couple of changes to the UI, mostly a result ofGnome 42 but nothing major that justified the praise it was getting. Don’t getme wrong, Gnome 42 is great but I had been using it for a while by the timeUbuntu 22.04 was released, so maybe that resulted in my lack of excitment.Anyway, a month or two after my initial experience with 22.04, I found myselfhaving to daily drive it. The circumstances leading to this are quiteunfortunate; very deserving of their own blog post. Here is a gist however,so I got this HP Spectre laptop for work some time back (before the release of22.04). My work to some extent does require using Windows. There is some stuffthat can only be done with Visual Studio (SSIS yikes). Windows being a must,I decided to dual boot Windows and Debian (Debian Testing - my OS of choice).Unfortunately for me, this laptop refused to work with Debian and every otherLinux distro I tried. Linux was unable to detect the SSD in the Laptop, I thinkdue to Intel Optane (the SSD has this tech that tries to boost Windows’performance - apparently just having an SSD is not enough for Windows, itsomehow needs more). Being inexperienced with this wonderful tech from Intel,I was unable to figure out how to turn off Intel Optane (I was looking for atoggle for this in the laptop’s BIOS menu - unfortunately it’s not there),so I gave up and just accepted my fate. I was using Windows (an OS I havevery little experience outside of playing pirated games) for work even thoughI only needed it for some 5% of my work and for the Linux stuff which is mostof the stuff, I used WSL. Fortunately, one day I figured out a way to getGNU/inux running and unfortunately for me, only Ubuntu 22.04 worked quite right.The Ubuntu ride hasn’t been smooth so far, there have been a ton of issues.I will list some of them down here and maybe provide some work arounds. Unfortunately, I left this in draft over 6 months ago, and I have moved on nowbut I am going try to fill in the gaps I left in the following sections.Applications getting OOMD to deathUbuntu has this systemd service calledsystem-oomd(Out of Memory Daemon). This service kills processes when you hit a certainmemory threshold. In Ubuntu, that threshold is set to half of the availableswap space. Of criteria that OOMD uses to kill processes, I am not so sure but fromobservation, I would guess that it goes for the processes using the most memory.OOMD on its own isn’t a problem in my opinion. However, when combined with Ubuntu’sdefaults, it becomes a pain. So, Ubuntu out of the gate these days, creates a swapfile that’s 2 GB in size. Open a couple of tabs in your favourite browser, have vscoderunning on the side, and mix it up with a couple Java services (elasticsearch and co),you will quickly exhaust your 8GB/16GB memory. Next thing you know, you are past thethe swap space threshold that OOMD is looking for. It then starts killing processeswithout you getting notified in any way. You &lt;ALT-TAB&gt;, only to find that thebrowser you had open or the editor you had open is gone. Seriously pissed me off, this.It took me a while to figure out what exactly was happening but when I learnt of this,I had two potential (easy) solution. I could either just disable swap or expand myswap space to a big enough size that I never get to use half of it. I went with thelatter because I only have 16GB of memory available and every once in a while whenI work I do get pretty close or go slightly above the 16GB. In order to increaseyour swap space, you would have to trash the swapfile in use and manually createa new bigger swapfile. For me, 8GB was more than enough.$ sudo swapoff$ sudo dd if=/dev/zero of=/swapfile bs=1M count=8192$ sudo mkswap /swapfile$ sudo swaponI got locked out because I did not touch her for 5 minutesThis is another weird one, I think it was caused by a rogue Gnome shellextension but I am not sure. I simply learnt to live with it and never botheredlooking for a permanent fix (upgrading to 22.10 did fix it though, eventually).What was happening was … every once in a while when the screen locks andI try to log back in, the lock screen would freeze. Apparently, it was stillregistering the keys I was hitting but for some reason the whole login processwould take some seconds or worse a minute. So, after entering my passwordthen hitting enter and waiting for some time, the login would succeed. I neverhad the patience to wait for the login, so I would simply hit &lt;CTRL-ALT-F1&gt;to switch to tty1 where there is a gdm login screen available, that alwaysworked.She is too shy to scream out loadI don’t recall exactly what this problem was but I am guessing it must havebeen a processor throttling issue. After a fresh install of Ubuntu 22.04, mylaptop (HP Spectre) was running at full tilt, max boost clock speedsfull time. The laptop burned so much, I could have used it to iron my clothes.After trying out a number of solutions I landed onthermald. I justsudo apt install thermald and then rebooted. The spectre was now as coolas ice. All was seemingly well after that until I tried to run someservices in minikube. I had these services running without issue beforeI installed thermald. Strangely, after installing thermald these serviceswould just hang at start up and then minikube would murder them for failingtheir probe checks. After digging a little deeper, I figured the problemwas the CPU limits I had set for these services. I had some of these servicesset to 500 millis (half a CPU) or so. So before thermald half a CPU on theSpectre was loosely half the boost clock (well over 1GHZ). After installingthermald, the CPU no longer boosted at all, so it was locked at something1.3GHZ max (my processor is Intel(R) Core(TM) i7-1065G7 which is clocked1.3GHZ base). Half of that is quite a low number that the servicesfound too low to function correctly under.The quick fix was to simply have the services run unconstrained. This slightlydid make my work difficult as the limits I had set were following productionvalues. I was simpy copying the configuration file I was using in productionand using it for my local development setup. I eventually created a tool thatallowed me to easily parameterize these values and have different valuesconfiguration paramters in production and local development setup (ShamelessI know but here it is anyway). Upgrading toUbuntu 22.10 did permanently resolve that lack of turbo-ing issue.In case you are wondering how I get the MHz that my CPU is currentlyrunning at, I run the following command:$ watch -n.1 \"cat /proc/cpuinfo | grep '^cpu MHz'\"" }, { "title": "You Can't Eat Your Pie And Have it Too", "url": "/posts/you-cant-eat-your-pie-and-have-it-too/", "categories": "Databases", "tags": "databases, distributed systems", "date": "2022-05-24 21:00:00 +0200", "snippet": "Get your CAP on and take a walk with meIf you have worked long enough with databases or distributed systems, chances are thatyou are familiar with the CAP theorem.CAP is short for the following ter...", "content": "Get your CAP on and take a walk with meIf you have worked long enough with databases or distributed systems, chances are thatyou are familiar with the CAP theorem.CAP is short for the following terms: Consistency All reads reflect the most recent writes regardless of which of partition ofa system was written to and where the read has occured. NOTE: This is differentfrom consistency in ACID of databasetransactions. Availability All reads are guaranteed to have data but they are not guaranteed to be consistentas above (that is, the data you get may not be of the most recent writes thesystem). Partition tolerance A system remains operational regardless of delays or errors in routing of messagesbetween partitions. Partitioning here refers toNetwork Partitioning in caseyou are having trouble with the way this term is being used. For more details refer to this Wikipediaarticle. Basically, CAP theorem tells us that it’s not possible to have a system that hasall three properties at the same time. You can only combine two of these properties inany system. For example, in a distributed system you are forced to make one of twochoices: (1) you can only have Consistency and Partition Tolerance (CP) or (2) you canonly have Availability and Partition tolerance (AP). This is due to the fact that bytheir very nature distributed systems have to be resilient tonetwork partitioning.The moment you go for a distributed system, you have implicitly made the choice of havingpartition tolerance hence you are left with picking one thing between availabilityand consistency.There are however limitations to the application of CAP in distributed systemsespecially in the world of databases. Availability is avery hard targetto hit even though the theory tells us otherwise. In theory, you can have a systemthat’s always available but in practice humans are highly error prone. They create bugslike nobody’s business and misconfigure systems for breakfast whilst existing in a worldwhere ESCOM is running the show. Thesefactors are the main culprits when it comes to system downtime regardless of anyavailability guarantees coming from your architectural choices.In addition, CAP doesn’t really reflect all the decisions people make when choosinga (NoSQL) database. To a large extent, people look at what their usage patterns on thedatabase will be. For example, you may be thinking about things like “will I primarily bedoing reads on this thing and do I need an immediate response everytime?” It’s that kindof stuff that largely dominates these decisions. CAP in this case is not enough.Take a seat, remove your CAP (show some respect) and have this PIE with meFrom the previous section, we know that CAP theorem may not be the best guiding principlewhen making the choice of what NoSQL database technology to use. I didn’t provide anyinformation as to why this applies to NoSQL database, let me explain a tiny bit (sorryfuture self you used to be quite lazy - I suspect you still are). The existence of NoSQLdatabases is to some extent from a need to enable easy horizontal scaling of storage. Thisis in contrast to the traditional relational databases which mostly scale well vertically.If you need more performance or more space, get a bigger box. You don’t really have muchleeway in terms of adding more boxes. Thus CAP is something you will hear a lot of in theNoSQL world.An alternative theory called PIE was proposed in order deal with the shortcomings ofCAP. You could think of it as something that supercedes CAP but I prefer to look atit as more of a complement. Now, let’s look at why PIE may matter more than CAP… Pattern flexibility You can throw any kind of question to a database and you should be able to pullout an answer without modifying your data model. This is something you willtypically see in a well normalised relational database. Databases with thisproperty are well suited for bothtransactional andanalytical uses. Infinite Scaling Need more storage and performance? Add more boxes to your cluster without limit(in theory). Your traditional relational database can’t really do this or theycan but the limits to this aren’t that far away. You will mostly see databaseswith this property in the data lake/warehouse space. Efficiency A second is too long a wait for your query responses. You need your queries tobe fast. Typical of transactional workloads and not too much of an in issue inanalytical workloads. A report that takes 5 minutes can be acceptable, howeverhaving your users wait for 10 seconds just to fetch a list of their friends onyour social network is a big NO. With PIE as well, you have to make a decision of picking two out of those threeproperties. Let’s go through the various combinations of these properties.Pattern flexibility &amp;&amp; Efficiency (PE)PE is in my opinion the most common combination (at least it should be for most use caseswhich are quite small scale). You want the fastest possible queries and at the same timeyou want to be in a position to ask questions that you didn’t have when you first cameup with your data model without making any changes to it. Typically, for you to achievethis you will model your data in a relational way (I don’t think we have much else betterthan this) and even if you don’t you will end up with a very general data model that’spacked in such a way that allows you to efficiently combine the various elements of yourdata model in arbitrary ways. However, this will present a challenge when you try to gopast one box. Think of a situation where you need to perform something akin to an SQLjoin on two datasets that span multiple computers. This is not easy, and it will come atthe cost of efficiency when done violating your objective in the first place.Workflows that take advantage of this are mostly transactional and analytical in thebasic Business Intelligence(BI)sense. The BI workflows here utiliseOLAP cube-like data models (egStarand Snowflake schemas.Pattern flexibility &amp;&amp; Infinite scaling (PI)From the section above, you have seen how difficult it is to scale beyond one box andretain something resembling efficiency. However, in some cases you will be willing to giveup some efficiency to gain the ability to ask adhoc questions of your databases. This issomething that you will typically see in some kinds of analytical workloads. Data lakesmostly fall under this combination. Data in data lakes is not organised in a way to allowfor efficient access. You may even find data in a data lake in different formats, somein CSV files and some just raw JSON objects stored in some object storage. To get answersout of this data you are pretty much looking at distributed computing tools like Sparkand Hadoop.Infinite Scaling &amp;&amp; Efficiency (IE - not Internet Explorer, please)To get IE, objects in the database have to be packed in such a way as to minimise theneed for complex computations when retrieving data. You in some way end up with a singledataset unlike in the extreme case of relational databases where you have multipledatasets (think of a table as a dataset). This enables you to partition this data acrossmultiple machines and have these machine independently operate on the parts of the datathey are storing. This structuring of data does however come at a cost. You may not beable to easily answer certain kinds of questions without modifying your data model.Workloads that take advantage of IE are mostly transactional in nature. Think of whenyou have an application making a lot of writes but you also need to read back that dataas fast as possible. You would probably use something like MongoDB.I am tired now, I must end thisChoosing a database for your application isn’t a simple thing. There are lots of thingsyou need to take into account. I have tried to explain some of these things in thecontext of CAP theorem and PIE theory. These notes mostly are on NoSQL databases but thereare some drops of wisdom here and there on relational databases as well. So long…References AWS re:Invent 2018: Building with AWS Databases: Match Your Workload to the Right Database (DAT301) I definitely do recommend this video, please watch if you want to learn a lot moreon this subject. Don’t settle for eventual consitency Why PIE theorem is more relevant than CAP theorem CAP Theorem for Databases: Consistency, Availability &amp; Partition Tolerance PIE Theorem Notes By Nikola Kovačević More people should be doing this, publish your notes please… The billion Wikipedia links I have left in the article" }, { "title": "Of Recursive Spells And Iterative Spirits", "url": "/posts/Of-Recursive-Spells-And-Iterative-Spirits/", "categories": "Programming", "tags": "programming, functional, lisp, scheme, magic, witchcraft", "date": "2022-05-14 00:00:00 +0200", "snippet": "Programs are like magic spells, they conjure the spirits that roam thedigital world (the Matrix). We formally know these spirits by the nameprocesses. I mostly think of my spells as a sequence of s...", "content": "Programs are like magic spells, they conjure the spirits that roam thedigital world (the Matrix). We formally know these spirits by the nameprocesses. I mostly think of my spells as a sequence of steps that a spiritneeds to carry out but this is not 100% true. The spirits don’t exactly followline by line what I ask of them. They merely take my instructions as guidelinesto what tune they dance. Of course, this is mostly all down to the mediums whoare responsible for translating my spells into a language the spiritsunderstand. The mediums sometimes do optimise my instructions for some selfishreasons (saving time and space I am told) and sometimes there just isn’t a oneto one translation of my instructions into the language spoken within the matrix.Out of the Recursive Spell Came the Recursive SpiritOf particular interest to me at this point in time is a class of spellsthat are called recursive. The general property of recursive spells is thatthey somehow directly or indirectly invoke themselves. Consider the popularspell ‘factorial’ that is presented in almost every recursion 101 lesson.What this spell does is: given a positive integer N, multiply all numbersfrom 1 to up to N. One of the ways of achieving this in a spell is to simplystart from N then multiply that N by the result of invoking the spell againon N - 1 until N bottoms down to 1. This simple spell can be specified in thelanguage of the old patriarchs (achinaSussman) as follows:(define (factorial n) (if (= n 0) 1 (* (factorial (- n 1)))) (factorial 5)) The spell above is specified in a language called Scheme (a dialectof Lisp). Went with Scheme here because it is very close to the languagethe Spirits understand (I am lying through my teeth but work with me here,you will soon see what I am on about). A Scheme spell is basically justa list of lists which when pushed into the matrix turns to a spirit whosepurpose in life is to decay into a single scalar value by continuouslyapplying the first element of every list (theCAR) onto the rest of list(the CDR - pronounced Couldar)to produce a scalar value or another list (that is more elementarycompared to the original - process repeats until this list becomes a singleatomic value).Now, when the spell above is pushed into the matrix through a medium likes9, it produces a spirit thatwhose life goes as follows:=&gt; (factorial 5)(* 5 (factorial 4))(* 5 (* 4 (factorial 3)))(* 5 (* 4 (* 3 (factorial 2))))(* 5 (* 4 (* 3 (* 2 (factorial 1)))))(* 5 (* 4 (* 3 (* 2 (* 1 (factorial 0))))))(* 5 (* 4 (* 3 (* 2 (* 1 1)))))(* 5 (* 4 (* 3 (* 2 1))))(* 5 (* 4 (* 3 2)))(* 5 (* 4 6))(* 5 24)=&gt; 120The thing to note about the spirit above is that overtime it grows inspace (ie the depth of the list grows first before it startsto reduce). That’s essentially what makes a recursive spirit (process).We started with a recursive spell and we ended up with a recursive spell. In the real world, think C or similar languages, recursive processeshave this growth in space from using more and more of the stack. Everyrecursive invocation pushes the stack down to make space for the localvariables of this new function invocation.An Iterative Spirit from a Recursive Spell, you say?Let’s rewrite our recursive spell in a slightly different way:(define (factorial2 n product) (if (= n 0) product (factorial2 (- n 1) (* n product))) (factorial2 5 1))This new factorial will decay as follows:=&gt; (factorial2 5 1)(factorial2 (- 5 1) (* 1 5))(factorial2 (- 4 1) (* 5 4))(factorial2 (- 3 1) (* 20 3))(factorial2 (- 2 1) (* 60 2))(factorial2 (- 1 1) (* 120 1))(factorial2 0 120)=&gt; 120Notice that in this iteration of the factorial spell, the original list westarted with did not grow in depth between recursive calls. This lack ofgrowth in space is characteristic of what is called is an iterative spirit.We started with what is obviously a recursive spell but we somehow ended upwith an iterative spirit instead of a recursive one. We are able to generatean iterative spirit from a recursive spell by specifying our spell in a stylethat is called tail call recursion.You can loosely think of tail call recursion as a recusion in which the very lastoperation in the spell is an invocation of the spell. Comparing our twospells, we will notice that factorial’s last operation is to applythe * operator whilst factorial2’s last operation is a call to itself. Not all mediums (compilers and interpretors) are smart enough to detecta tail call recursion and optimise it into an iterative spirit. Most mediumsthat are able to do this, reside in the functional world. Tail calloptimisation is necessary within the functional because the majority oflanguages in that world have no looping constructs like for-statements.All looping is done through recursion (even where a language providessomething akin to a for-statement, it’s usually just a macro that turnsinto a recursive call).Parting wordsI am not really good at drawing conclusions but if anything the thingyou should take from this is: Programs generate processes Recursive programs usually generate recursive processes Recursive programs written in a tail call style generate iterative processesif the compiler/interpretor in use supports it. Iterative processes are more efficient in their use of spaceReferences Structure and Interpretation of Computer Programs Been a long time since I last read this but I definitely do recommend Sketchy Lisp Wikipedia" } ]
